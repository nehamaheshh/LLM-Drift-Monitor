# ðŸš¦ LLM Drift Monitor

A practical system to monitor how Large Language Models (LLMs) **change their behavior over time or across models** â€” even when answers still appear correct.

Instead of focusing on accuracy, this project answers a more important production question:

> **Is the model still behaving the way we expect?**

---

## ðŸ” What This Project Does

This system logs LLM interactions and detects different types of drift:

### 1ï¸âƒ£ Semantic Drift (Meaning)
Checks whether the *meaning* of responses is changing using embedding similarity.

### 2ï¸âƒ£ Structural Drift (Style & Verbosity)
Detects changes in:
- response length  
- verbosity and formatting  

using statistical tests on response length distributions.

### 3ï¸âƒ£ Safety Drift
Tracks changes in refusal behavior (for example, when a model starts refusing more queries).

### 4ï¸âƒ£ Cost Drift (Estimated)
Estimates token usage and cost based on response length to detect silent cost increases.

### 5ï¸âƒ£ Drift Over Time
Uses rolling windows to show **when** behavior changes started.

### 6ï¸âƒ£ Auto Alerts
Summarizes drift into:
- ðŸŸ¢ **PASS** â€“ normal variation  
- ðŸŸ¡ **WARN** â€“ moderate drift  
- ðŸ”´ **ALERT** â€“ significant behavior change  

---

## ðŸ§  Why This Matters

In real-world LLM systems:
- Accuracy may stay stable
- But responses can become longer, slower, more restrictive, or more expensive

These changes often go unnoticed until they impact users or cost.  
This project shows why **LLM evaluation needs more than accuracy**.

---

## ðŸ§ª Example Insight

A controlled experiment comparing **LLaMA-3-8B** and **Qwen-2.5-7B** showed:

- Meaning stayed mostly the same
- Response length changed significantly
- Refusal behavior increased
- Estimated cost profile shifted

Traditional evaluation would miss this drift.

---

## ðŸ§± Tech Stack

- **LLM Runtime:** Ollama  
- **Models:** LLaMA-3-8B, Qwen-2.5-7B  
- **Embeddings:** sentence-transformers (MiniLM)  
- **Database:** SQLite  
- **Statistics:** SciPy (KS test)  
- **Dashboard:** Streamlit  
- **Language:** Python  

---

## ðŸ“‚ Repository Structure

llm-drift-monitor/
â”œâ”€ dashboard/
â”‚ â””â”€ app.py
â”œâ”€ data/
â”‚ â””â”€ llm_logs.db # ignored by git
â”œâ”€ reports/
â”‚ â””â”€ .gitkeep
â”œâ”€ scripts/
â”‚ â”œâ”€ run_prompt.py
â”‚ â”œâ”€ run_daily_monitor.py
â”‚ â”œâ”€ run_model_compare.py
â”‚ â”œâ”€ check_counts.py
â”‚ â””â”€ inspect_experiments.py
â”œâ”€ src/
â”‚ â”œâ”€ drift/
â”‚ â”‚ â””â”€ detectors.py
â”‚ â”œâ”€ features/
â”‚ â”‚ â”œâ”€ embed.py
â”‚ â”‚ â””â”€ text_features.py
â”‚ â”œâ”€ llm/
â”‚ â”‚ â””â”€ ollama_client.py
â”‚ â””â”€ logging/
â”‚ â”œâ”€ logger.py
â”‚ â””â”€ schema.py
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â”œâ”€ requirements.txt
â””â”€ README.md

yaml
Copy code

---

## ðŸš€ Quickstart

### 1ï¸âƒ£ Create & activate virtual environment (Windows PowerShell)

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```
### 2ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```
### 3ï¸âƒ£ Start Ollama & pull models
```
ollama list
ollama pull llama3:8b
ollama pull qwen2.5:7b
```
### 4ï¸âƒ£ Log LLM interactions
This logs prompts, responses, embeddings, latency, length, and refusal flags.
```
python -m scripts.run_prompt
```
### 5ï¸âƒ£ Run CLI drift monitor
```
python -m scripts.run_daily_monitor
```
### 6ï¸âƒ£ Launch Streamlit dashboard
```
streamlit run dashboard/app.py
```
ðŸ§ª Controlled Model Switch Experiment (Equal Samples)
This performs a clean A/B test:

-Same prompts
-Same sample size
-Only model changes

```
$env:EXPERIMENT_ID="model_switch_equal_samples"

$env:OLLAMA_MODEL="llama3:8b"
1..31 | % { python -m scripts.run_prompt }   # â‰ˆ93 samples

$env:OLLAMA_MODEL="qwen2.5:7b"
1..31 | % { python -m scripts.run_prompt }

python -m scripts.check_counts
python -m scripts.run_model_compare
```

Expected outcome:

âœ… Mild semantic drift

âš ï¸ Strong verbosity / length drift

âš ï¸ Possible refusal-rate changes

ðŸ“Š Dashboard Overview
Tabs included:
-Drift Snapshot â€“ current baseline vs recent behavior
-Drift Over Time â€“ rolling-window semantic & length drift
-Model Compare â€“ equal-sample A/B comparison
-Recent Logs â€“ raw interaction inspection

Auto Alert System
Each comparison is labeled:

-ðŸŸ¢ PASS â€“ normal variation
-ðŸŸ¡ WARN â€“ moderate drift detected
-ðŸ”´ ALERT â€“ statistically significant behavior change

Alerts are triggered using:

-semantic drift thresholds
-KS statistic + p-value
-refusal-rate increase
-estimated cost increase

ðŸ“ Metrics & Interpretation
Semantic Drift (Cosine Distance)
Range	Meaning
0.00 â€“ 0.03	Tiny
0.03 â€“ 0.08	Mild
0.08 â€“ 0.15	Moderate
> 0.15	High

Structural Drift (KS Test)
-KS statistic â†‘ â†’ larger distribution shift
-p < 0.01 â†’ statistically significant drift

Why this matters
Even when semantic content stays stable, drift in:

-verbosity
-latency
-refusal behavior
-token cost

can silently degrade UX, increase spend, or break workflows.

ðŸ§  Key Insight Demonstrated
Switching from LLaMA-3-8B to Qwen-2.5-7B caused major verbosity and refusal-rate drift while semantic meaning remained largely stable â€” showing why accuracy alone is insufficient for LLM evaluation.

ðŸ”’ Notes
-data/llm_logs.db is intentionally not committed
-Use EXPERIMENT_ID to keep experiments isolated and reproducible
-Cost estimates are approximate (token â‰ˆ 1.33 Ã— words)



